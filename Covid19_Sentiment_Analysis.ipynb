{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Covid19-Sentiment Analysis.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMihjmvMxccq/BhcLSlIMW1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akhilnooney/Akhil/blob/master/Covid19_Sentiment_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q2EK2NbZGSdp",
        "colab_type": "code",
        "outputId": "078439ac-af0a-4e4a-a95a-04ab0eb20803",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "source": [
        "pip install tweepy"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tweepy in /usr/local/lib/python3.6/dist-packages (3.6.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tweepy) (1.3.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tweepy) (1.12.0)\n",
            "Requirement already satisfied: PySocks>=1.5.7 in /usr/local/lib/python3.6/dist-packages (from tweepy) (1.7.1)\n",
            "Requirement already satisfied: requests>=2.11.1 in /usr/local/lib/python3.6/dist-packages (from tweepy) (2.21.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->tweepy) (3.1.0)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.11.1->tweepy) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.11.1->tweepy) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.11.1->tweepy) (2019.11.28)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.11.1->tweepy) (3.0.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wcWh0LNlGZFM",
        "colab_type": "code",
        "outputId": "ea344694-e7f6-4932-a43a-b3f0f8b33bc1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "pip install geotext"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting geotext\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/25/c5/36351193092cb4c1d7002d2a3babe5e72ae377868473933d6f63b41e5454/geotext-0.4.0-py2.py3-none-any.whl (2.0MB)\n",
            "\u001b[K     |████████████████████████████████| 2.0MB 2.8MB/s \n",
            "\u001b[?25hInstalling collected packages: geotext\n",
            "Successfully installed geotext-0.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJYP2GC0Geiu",
        "colab_type": "code",
        "outputId": "20a3a55a-f5e2-4b38-9e57-07e0371c7bbd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 433
        }
      },
      "source": [
        "\n",
        "import tweepy\n",
        "import pandas as pd\n",
        "from geotext import GeoText\n",
        "# API credentials here\n",
        "consumer_key = 'hi3NdqgmcADqcZPRtHYxVxgZz'\n",
        "consumer_secret = '3GkNzsZtQTpxl32jMm9Pg6YwustmvlRdzQXQ4GhwXQvTr1BObu'\n",
        "access_token = '815976441619693568-XapVuD7GFviOKanvhCZOFXLA5Yezs5G'\n",
        "access_token_secret = '93RUpa40OzRYZyTVemF9cy6hs8xShCSgv7X9UcUmE5yx3'\n",
        "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
        "auth.set_access_token(access_token, access_token_secret)\n",
        "api = tweepy.API(auth,wait_on_rate_limit=True,wait_on_rate_limit_notify=True)\n",
        "searchString = '#covid_19' or '#covid-19'\n",
        "cursor = tweepy.Cursor(api.search, q=searchString, count=20, lang=\"en\", tweet_mode='extended')\n",
        "tweets=[]\n",
        "maxCount = 3000\n",
        "count = 0\n",
        "df = pd.DataFrame({'id': [], 'Created_at': [], 'Tweet': [],'Location':[]})\n",
        "for tweet in cursor.items():    \n",
        "    if(tweet.user.location!=''):\n",
        "      places = GeoText(tweet.user.location)\n",
        "      df = df.append({'id': tweet.id, 'Created_at': tweet.created_at, 'Tweet':tweet.full_text,'Location':places.countries}, ignore_index=True)\n",
        "      count = count + 1\n",
        "      if(count == maxCount):\n",
        "        break;\n",
        "df"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Rate limit reached. Sleeping for: 853\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>Created_at</th>\n",
              "      <th>Tweet</th>\n",
              "      <th>Location</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.246565e+18</td>\n",
              "      <td>2020-04-04 22:25:25</td>\n",
              "      <td>RT @WarRoomPandemic: Dr. Grace: Most #NYC hosp...</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.246565e+18</td>\n",
              "      <td>2020-04-04 22:25:21</td>\n",
              "      <td>RT @MichaelTCucek: #JapanSociety - the country...</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.246564e+18</td>\n",
              "      <td>2020-04-04 22:25:10</td>\n",
              "      <td>Our Executive Director, @eedgerly, wanted to u...</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.246564e+18</td>\n",
              "      <td>2020-04-04 22:25:06</td>\n",
              "      <td>RT @medmalinf: [#COVID19] No Evidence of Rapid...</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.246564e+18</td>\n",
              "      <td>2020-04-04 22:24:57</td>\n",
              "      <td>Shopaholic? Here's your fix.\\nsanipoe 360 Make...</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2995</th>\n",
              "      <td>1.246520e+18</td>\n",
              "      <td>2020-04-04 19:26:41</td>\n",
              "      <td>Here's some of the information on JCAN that is...</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2996</th>\n",
              "      <td>1.246520e+18</td>\n",
              "      <td>2020-04-04 19:26:40</td>\n",
              "      <td>RT @ramichuene: I’m not trying to start a figh...</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2997</th>\n",
              "      <td>1.246520e+18</td>\n",
              "      <td>2020-04-04 19:26:39</td>\n",
              "      <td>RT @revathitweets: Goshamahal MLA from BJP @Ti...</td>\n",
              "      <td>[India]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2998</th>\n",
              "      <td>1.246520e+18</td>\n",
              "      <td>2020-04-04 19:26:39</td>\n",
              "      <td>RT @HarsimratBadal_: As I relive the mental ag...</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2999</th>\n",
              "      <td>1.246520e+18</td>\n",
              "      <td>2020-04-04 19:26:33</td>\n",
              "      <td>Online meeting/interaction/remote work solutio...</td>\n",
              "      <td>[Ghana]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3000 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                id  ... Location\n",
              "0     1.246565e+18  ...       []\n",
              "1     1.246565e+18  ...       []\n",
              "2     1.246564e+18  ...       []\n",
              "3     1.246564e+18  ...       []\n",
              "4     1.246564e+18  ...       []\n",
              "...            ...  ...      ...\n",
              "2995  1.246520e+18  ...       []\n",
              "2996  1.246520e+18  ...       []\n",
              "2997  1.246520e+18  ...  [India]\n",
              "2998  1.246520e+18  ...       []\n",
              "2999  1.246520e+18  ...  [Ghana]\n",
              "\n",
              "[3000 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__IJfKi6LazT",
        "colab_type": "code",
        "outputId": "4e9b516c-6264-49d3-9dcf-15f7c07a0b5f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NLTK Downloader\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> d\n",
            "\n",
            "Download which package (l=list; x=cancel)?\n",
            "  Identifier> l\n",
            "Packages:\n",
            "  [ ] abc................. Australian Broadcasting Commission 2006\n",
            "  [ ] alpino.............. Alpino Dutch Treebank\n",
            "  [ ] averaged_perceptron_tagger Averaged Perceptron Tagger\n",
            "  [ ] averaged_perceptron_tagger_ru Averaged Perceptron Tagger (Russian)\n",
            "  [ ] basque_grammars..... Grammars for Basque\n",
            "  [ ] biocreative_ppi..... BioCreAtIvE (Critical Assessment of Information\n",
            "                           Extraction Systems in Biology)\n",
            "  [ ] bllip_wsj_no_aux.... BLLIP Parser: WSJ Model\n",
            "  [ ] book_grammars....... Grammars from NLTK Book\n",
            "  [ ] brown............... Brown Corpus\n",
            "  [ ] brown_tei........... Brown Corpus (TEI XML Version)\n",
            "  [ ] cess_cat............ CESS-CAT Treebank\n",
            "  [ ] cess_esp............ CESS-ESP Treebank\n",
            "  [ ] chat80.............. Chat-80 Data Files\n",
            "  [ ] city_database....... City Database\n",
            "  [ ] cmudict............. The Carnegie Mellon Pronouncing Dictionary (0.6)\n",
            "  [ ] comparative_sentences Comparative Sentence Dataset\n",
            "  [ ] comtrans............ ComTrans Corpus Sample\n",
            "  [ ] conll2000........... CONLL 2000 Chunking Corpus\n",
            "  [ ] conll2002........... CONLL 2002 Named Entity Recognition Corpus\n",
            "Hit Enter to continue: \n",
            "  [ ] conll2007........... Dependency Treebanks from CoNLL 2007 (Catalan\n",
            "                           and Basque Subset)\n",
            "  [ ] crubadan............ Crubadan Corpus\n",
            "  [ ] dependency_treebank. Dependency Parsed Treebank\n",
            "  [ ] dolch............... Dolch Word List\n",
            "  [ ] europarl_raw........ Sample European Parliament Proceedings Parallel\n",
            "                           Corpus\n",
            "  [ ] floresta............ Portuguese Treebank\n",
            "  [ ] framenet_v15........ FrameNet 1.5\n",
            "  [ ] framenet_v17........ FrameNet 1.7\n",
            "  [ ] gazetteers.......... Gazeteer Lists\n",
            "  [ ] genesis............. Genesis Corpus\n",
            "  [ ] gutenberg........... Project Gutenberg Selections\n",
            "  [ ] ieer................ NIST IE-ER DATA SAMPLE\n",
            "  [ ] inaugural........... C-Span Inaugural Address Corpus\n",
            "  [ ] indian.............. Indian Language POS-Tagged Corpus\n",
            "  [ ] jeita............... JEITA Public Morphologically Tagged Corpus (in\n",
            "                           ChaSen format)\n",
            "  [ ] kimmo............... PC-KIMMO Data Files\n",
            "  [ ] knbc................ KNB Corpus (Annotated blog corpus)\n",
            "  [ ] large_grammars...... Large context-free and feature-based grammars\n",
            "                           for parser comparison\n",
            "Hit Enter to continue: \n",
            "  [ ] lin_thesaurus....... Lin's Dependency Thesaurus\n",
            "  [ ] mac_morpho.......... MAC-MORPHO: Brazilian Portuguese news text with\n",
            "                           part-of-speech tags\n",
            "  [ ] machado............. Machado de Assis -- Obra Completa\n",
            "  [ ] masc_tagged......... MASC Tagged Corpus\n",
            "  [ ] maxent_ne_chunker... ACE Named Entity Chunker (Maximum entropy)\n",
            "  [ ] maxent_treebank_pos_tagger Treebank Part of Speech Tagger (Maximum entropy)\n",
            "  [ ] moses_sample........ Moses Sample Models\n",
            "  [ ] movie_reviews....... Sentiment Polarity Dataset Version 2.0\n",
            "  [ ] mte_teip5........... MULTEXT-East 1984 annotated corpus 4.0\n",
            "  [ ] mwa_ppdb............ The monolingual word aligner (Sultan et al.\n",
            "                           2015) subset of the Paraphrase Database.\n",
            "  [ ] names............... Names Corpus, Version 1.3 (1994-03-29)\n",
            "  [ ] nombank.1.0......... NomBank Corpus 1.0\n",
            "  [ ] nonbreaking_prefixes Non-Breaking Prefixes (Moses Decoder)\n",
            "  [ ] nps_chat............ NPS Chat\n",
            "  [ ] omw................. Open Multilingual Wordnet\n",
            "  [ ] opinion_lexicon..... Opinion Lexicon\n",
            "  [ ] panlex_swadesh...... PanLex Swadesh Corpora\n",
            "  [ ] paradigms........... Paradigm Corpus\n",
            "  [ ] pe08................ Cross-Framework and Cross-Domain Parser\n",
            "                           Evaluation Shared Task\n",
            "Hit Enter to continue: \n",
            "  [ ] perluniprops........ perluniprops: Index of Unicode Version 7.0.0\n",
            "                           character properties in Perl\n",
            "  [ ] pil................. The Patient Information Leaflet (PIL) Corpus\n",
            "  [ ] pl196x.............. Polish language of the XX century sixties\n",
            "  [ ] porter_test......... Porter Stemmer Test Files\n",
            "  [ ] ppattach............ Prepositional Phrase Attachment Corpus\n",
            "  [ ] problem_reports..... Problem Report Corpus\n",
            "  [ ] product_reviews_1... Product Reviews (5 Products)\n",
            "  [ ] product_reviews_2... Product Reviews (9 Products)\n",
            "  [ ] propbank............ Proposition Bank Corpus 1.0\n",
            "  [ ] pros_cons........... Pros and Cons\n",
            "  [ ] ptb................. Penn Treebank\n",
            "  [ ] qc.................. Experimental Data for Question Classification\n",
            "  [ ] reuters............. The Reuters-21578 benchmark corpus, ApteMod\n",
            "                           version\n",
            "  [ ] rslp................ RSLP Stemmer (Removedor de Sufixos da Lingua\n",
            "                           Portuguesa)\n",
            "  [ ] rte................. PASCAL RTE Challenges 1, 2, and 3\n",
            "  [ ] sample_grammars..... Sample Grammars\n",
            "  [ ] semcor.............. SemCor 3.0\n",
            "  [ ] senseval............ SENSEVAL 2 Corpus: Sense Tagged Text\n",
            "Hit Enter to continue: \n",
            "  [ ] sentence_polarity... Sentence Polarity Dataset v1.0\n",
            "  [ ] sentiwordnet........ SentiWordNet\n",
            "  [ ] shakespeare......... Shakespeare XML Corpus Sample\n",
            "  [ ] sinica_treebank..... Sinica Treebank Corpus Sample\n",
            "  [ ] smultron............ SMULTRON Corpus Sample\n",
            "  [ ] snowball_data....... Snowball Data\n",
            "  [ ] spanish_grammars.... Grammars for Spanish\n",
            "  [ ] state_union......... C-Span State of the Union Address Corpus\n",
            "  [ ] subjectivity........ Subjectivity Dataset v1.0\n",
            "  [ ] swadesh............. Swadesh Wordlists\n",
            "  [ ] switchboard......... Switchboard Corpus Sample\n",
            "  [ ] tagsets............. Help on Tagsets\n",
            "  [ ] timit............... TIMIT Corpus Sample\n",
            "  [ ] toolbox............. Toolbox Sample Files\n",
            "  [ ] treebank............ Penn Treebank Sample\n",
            "  [ ] twitter_samples..... Twitter Samples\n",
            "  [ ] udhr2............... Universal Declaration of Human Rights Corpus\n",
            "                           (Unicode Version)\n",
            "  [ ] udhr................ Universal Declaration of Human Rights Corpus\n",
            "  [ ] unicode_samples..... Unicode Samples\n",
            "  [ ] universal_tagset.... Mappings to the Universal Part-of-Speech Tagset\n",
            "Hit Enter to continue: \n",
            "  [ ] universal_treebanks_v20 Universal Treebanks Version 2.0\n",
            "  [ ] vader_lexicon....... VADER Sentiment Lexicon\n",
            "  [ ] verbnet3............ VerbNet Lexicon, Version 3.3\n",
            "  [ ] verbnet............. VerbNet Lexicon, Version 2.1\n",
            "  [ ] webtext............. Web Text Corpus\n",
            "  [ ] wmt15_eval.......... Evaluation data from WMT15\n",
            "  [ ] word2vec_sample..... Word2Vec Sample\n",
            "  [ ] wordnet_ic.......... WordNet-InfoContent\n",
            "  [ ] words............... Word Lists\n",
            "  [ ] ycoe................ York-Toronto-Helsinki Parsed Corpus of Old\n",
            "                           English Prose\n",
            "\n",
            "Collections:\n",
            "  [P] all-corpora......... All the corpora\n",
            "  [P] all-nltk............ All packages available on nltk_data gh-pages\n",
            "                           branch\n",
            "  [P] all................. All packages\n",
            "  [P] book................ Everything used in the NLTK Book\n",
            "  [P] popular............. Popular packages\n",
            "  [ ] tests............... Packages for running tests\n",
            "  [ ] third-party......... Third-party data packages\n",
            "\n",
            "([*] marks installed packages; [P] marks partially installed collections)\n",
            "\n",
            "Download which package (l=list; x=cancel)?\n",
            "  Identifier> \n",
            "\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> \n",
            "\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> q\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XGGP-VDiL1sg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Tokenization\n",
        "def identify_tokens(row):\n",
        "    review = row['Tweet']\n",
        "    tokens = nltk.word_tokenize(review)\n",
        "    # taken only words (not punctuation)\n",
        "    token_words = [w for w in tokens if w.isalpha()]\n",
        "    return token_words"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmYiTWgdSfUX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Stemming\n",
        "from nltk.stem import PorterStemmer\n",
        "stemming = PorterStemmer()\n",
        "def stem_list(row):\n",
        "    my_list = row['Tweet']\n",
        "    stemmed_list = [stemming.stem(word) for word in my_list]\n",
        "    return (stemmed_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rDAYCOvCSlfk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Lemmatization\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "lemmatizer = WordNetLemmatizer()\n",
        "def lem_list(row):\n",
        "    my_list = row['Tweet']\n",
        "    lemmatized_list = [lemmatizer.lemmatize(word) for word in my_list]\n",
        "    return (lemmatized_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xEiLmq93SrOD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Removing stop words\n",
        "from nltk.corpus import stopwords\n",
        "stops = set(stopwords.words(\"english\"))                  \n",
        "\n",
        "def remove_stops(row):\n",
        "    my_list = row['Tweet']\n",
        "    meaningful_words = [w for w in my_list if not w in stops]\n",
        "    return (meaningful_words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LgKtPJBwSt8Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Rejoining Tweets\n",
        "def rejoin_words(row):\n",
        "    my_list = row['Tweet']\n",
        "    joined_words = ( \" \".join(my_list))\n",
        "    return joined_words"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f8yaxNKZSwrw",
        "colab_type": "code",
        "outputId": "c41d5158-2932-4855-930a-9c8387ee2c0d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 519
        }
      },
      "source": [
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "stop = stopwords.words('english')\n",
        "frame=df[(df.astype(str)['Location']!='[]')]\n",
        "frame['Tweet'] = frame['Tweet'].str.lower()\n",
        "frame = frame.drop_duplicates(subset='Tweet')\n",
        "frame[\"Tweet\"] = frame['Tweet'].str.replace('[^\\w\\s]','')\n",
        "frame['Tweet'] = frame.apply(identify_tokens, axis=1)\n",
        "frame['Tweet'] = frame.apply(stem_list, axis=1)\n",
        "frame['Tweet'] = frame.apply(lem_list, axis=1)\n",
        "frame['Tweet'] = frame.apply(remove_stops, axis=1)\n",
        "frame['Tweet'] = frame.apply(rejoin_words, axis=1)\n",
        "frame['Tweet'] = frame['Tweet'].str.replace(\"rt\",'')\n",
        "frame['Location']=frame['Location'].str.get(0)\n",
        "Top_countries=frame['Location'].value_counts()[:5].sort_values(ascending=False).index.tolist()\n",
        "frame=frame.loc[frame['Location'].isin(Top_countries)]\n",
        "frame"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \"\"\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>Created_at</th>\n",
              "      <th>Tweet</th>\n",
              "      <th>Location</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>1.246564e+18</td>\n",
              "      <td>2020-04-04 22:23:25</td>\n",
              "      <td>african leader confer call discus old french ...</td>\n",
              "      <td>South Africa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>1.246564e+18</td>\n",
              "      <td>2020-04-04 22:23:13</td>\n",
              "      <td>norbeelek sa man vavi show u video fight coron...</td>\n",
              "      <td>South Africa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>1.246564e+18</td>\n",
              "      <td>2020-04-04 22:21:32</td>\n",
              "      <td>adriandelmont yet want test vaccin africa peop...</td>\n",
              "      <td>South Africa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59</th>\n",
              "      <td>1.246563e+18</td>\n",
              "      <td>2020-04-04 22:21:00</td>\n",
              "      <td>mohammadkaif good hand ensur catch ball clean...</td>\n",
              "      <td>India</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>63</th>\n",
              "      <td>1.246563e+18</td>\n",
              "      <td>2020-04-04 22:20:46</td>\n",
              "      <td>thi best thing ive seen dure thi pandem phine...</td>\n",
              "      <td>United Kingdom</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2926</th>\n",
              "      <td>1.246520e+18</td>\n",
              "      <td>2020-04-04 19:29:32</td>\n",
              "      <td>meat eater risk sta danger diseas like corona...</td>\n",
              "      <td>United Kingdom</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2943</th>\n",
              "      <td>1.246520e+18</td>\n",
              "      <td>2020-04-04 19:28:55</td>\n",
              "      <td>dolphin pop surfac say hello nearli yr onli ca...</td>\n",
              "      <td>India</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2971</th>\n",
              "      <td>1.246520e+18</td>\n",
              "      <td>2020-04-04 19:27:34</td>\n",
              "      <td>drmurukada noconvers suggest dump dablighijam...</td>\n",
              "      <td>India</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2985</th>\n",
              "      <td>1.246520e+18</td>\n",
              "      <td>2020-04-04 19:27:02</td>\n",
              "      <td>seed inherit buri conscienc awesom youtub vide...</td>\n",
              "      <td>South Africa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2994</th>\n",
              "      <td>1.246520e+18</td>\n",
              "      <td>2020-04-04 19:26:44</td>\n",
              "      <td>sunidraindia sleep contestale coronaviru cont...</td>\n",
              "      <td>India</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>250 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                id  ...        Location\n",
              "23    1.246564e+18  ...    South Africa\n",
              "27    1.246564e+18  ...    South Africa\n",
              "49    1.246564e+18  ...    South Africa\n",
              "59    1.246563e+18  ...           India\n",
              "63    1.246563e+18  ...  United Kingdom\n",
              "...            ...  ...             ...\n",
              "2926  1.246520e+18  ...  United Kingdom\n",
              "2943  1.246520e+18  ...           India\n",
              "2971  1.246520e+18  ...           India\n",
              "2985  1.246520e+18  ...    South Africa\n",
              "2994  1.246520e+18  ...           India\n",
              "\n",
              "[250 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wm-JuHf6vYHE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}